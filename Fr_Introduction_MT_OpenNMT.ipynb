{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwYtIqx7VMjL"
      },
      "source": [
        "# Introduction à MT avec OpenNMT.\n",
        "\n",
        "![OPENNMT](https://avatars.githubusercontent.com/u/23035727?s=200&v=4)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\"https://github.com/s7d11/indabax-practicals\")\n",
        "\n",
        "Présenté par:\n",
        "\n",
        "- Sebastien Diarra, Michael Leventhal\n",
        "\n",
        "Procédure :\n",
        "\n",
        "- IndabaX Mali 2022\n",
        "\n",
        "## Agenda\n",
        "- À savoir\n",
        "- Introduction\n",
        "- Prérequis\n",
        "- Configuration de l'Environnement\n",
        "- Préparation des données\n",
        "- Architecture du modèle\n",
        "- Entraînement\n",
        "- Essai & Évaluation des performances [BLEU, Tensorboard]\n",
        "- Questions et Réponses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHNJcDVFQ1qG"
      },
      "source": [
        "\n",
        "## Buts et objectifs\n",
        "- Pour que vous compreniez le MT. Pipeline\n",
        "- Apprenez à configurer l'environnement OpenNMT\n",
        "- Pour vous intéresser à notre initiative Bayɛlɛmabaga & MT pour les langues locales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKYLP6Hbjh4v"
      },
      "source": [
        "**IMPORTANT REMARKS**: \n",
        "\n",
        "- This practical is intented for those with no background in MT or ML for that matter.\n",
        "- Pratical is Focused more on doing rather telling.\n",
        "- Hopefully you'll leave this session with an eager to contribute to MT work for Mali.\n",
        "    - You do not need to be an expert to contribute. You can contribute Data !\n",
        "\n",
        "*NB*: Join someone with a computer if did not bring your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHKbVDEBQ1qI"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Dans cette pratique, nous découvrirons OpenNMT et comment former un modèle de traduction automatique bilingue pour le bambara - français. Ceci est un tutoriel d'introduction pour se mouiller les pieds. Une approche pratique est fortement recommandée, tous les participants sont encouragés à suivre. Le code source de cet atelier peut être exécuté via Colab à l'URL suivante :\n",
        "\n",
        "[https://github.com/robotsmali-ai/rmai/]()\n",
        "\n",
        "\n",
        "### [OpenNMT](https://opennmt.net/)\n",
        "\n",
        "\"OpenNMT - est un écosystème open source pour la traduction automatique neuronale et l'apprentissage des séquences neuronales\"\n",
        "\n",
        "C'est un framework, ou toolkit, qui permet d'entrainer un modèle de langage.\n",
        "\n",
        "OpenNMT est pour :\n",
        "- Débutants\n",
        "- Amateur\n",
        "- Experts\n",
        "\n",
        "### MT (Traduction Automatique)\n",
        "\n",
        "Processus automatisé de traduction d'une langue à une autre par machine sans intervention humaine. La traduction est généralement effectuée par un programme informatique. Il existe différents types de MT :\n",
        "\n",
        "- MT basée sur des règles : règles de grammaire et de langue utilisées comme base pour un logiciel pour effectuer la traduction.\n",
        "\n",
        "![RULE-IMAGE](https://i.ibb.co/7XbQV0H/grammar-rules.png)\n",
        "\n",
        "- SMT : MT statistique : apprentissage basé sur la distribution de probabilité.\n",
        "\n",
        "\n",
        "<div>\n",
        "<img alt=\"SMT\" width=\"300\"; height=\"250\" src=\"https://www.researchgate.net/profile/Andy-Way/publication/220418877/figure/fig5/AS:669019598245898@1536518110475/A-Statistical-Machine-Translation-System-adapted-from-Figure-1-in-Brown-et-al-13.png\" />\n",
        "\n",
        "<sub>Courtesy of: https://www.researchgate.net/publication/220418877_Hybrid_data-driven_models_of_machine_translation</sub>\n",
        "</div>\n",
        "\n",
        "- NMT : Neural Machine Translation : Apprentissage basé sur les réseaux de neurones artificiels.\n",
        "\n",
        "**NMT : c'est ce qui nous intéresse dans cet atelier**\n",
        "\n",
        "### NMT\n",
        "\n",
        "Est-ce l'utilisation de techniques d'apprentissage en profondeur (réseaux de neurones) pour former un modèle de traduction. Cette approche est devenue l'approche dominante pour faire de la traduction automatique. NMT s'est avéré très efficace dans la traduction automatique ces dernières années.\n",
        "\n",
        "<div>\n",
        "<img width=\"300\" height=\"250\" src=\"https://1.cms.s81c.com/sites/default/files/2021-01-06/ICLH_Diagram_Batch_01_03-DeepNeuralNetwork-WHITEBG.png\">\n",
        "</div>\n",
        "\n",
        "OpenNMT -- nous aide à expérimenter rapidement la NMT, ainsi qu'à créer des systèmes/solutions de MT en moins de temps.\n",
        "\n",
        "NMT englobe les deux autres approches dans une certaine mesure.\n",
        "\n",
        "**NB** : ***Venez à l'événement PNL samedi, pour apprendre ces concepts en détail.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc_4xo9vaYN4"
      },
      "source": [
        "## Prérequis\n",
        "- Compréhension de la CLI Unix (aide)\n",
        "- Compréhension de Python (JupyterNB)\n",
        "- Les concepts de base de ML (tels que Pipeline, Models, NN) aident mais ne sont pas obligatoires\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3l7XwllQ1qL"
      },
      "source": [
        "## Configuration de l'Environnement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcysJBiGQ1qM"
      },
      "source": [
        "### Paquets\n",
        "\n",
        "- `OpenNMT-py` : paquet opennmt principal (version pytorch)\n",
        "- `rmaipkg` : Package pour les ensembles de données et les modèles de RobotsMaliAI (Later)\n",
        "- `Daba` (non requis) : une version modifiée de [daba](https://github.com/maslinych/daba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpEX2wSJQ1qN",
        "outputId": "c92e9f78-04ce-4c53-d262-54460808c2c0"
      },
      "outputs": [],
      "source": [
        "%pip install OpenNMT-py subword_nmt sentencepiece # Main OpenNMT Package (No Need for subword_nmt sentencepiece)\n",
        "%pip install --no-cache-dir https://github.com/RobotsMali-AI/rmai/releases/download/0.0.3/rmaipkg-0.0.3.tar.gz # RobotsMaliAI's Datasets and Models\n",
        "%pip install -U https://github.com/s7d11/daba/releases/download/v0.0.1-alpha/daba-0.9.2.tar.gz # Non-UI Version of Daba\n",
        "%pip install sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvsF2lYkQ1qR"
      },
      "source": [
        "### Configuration de l'espace de travail (Google Colab)\n",
        "\n",
        "- Qu'est-ce que Colab ?\n",
        "\n",
        "- Local vs Google Drive\n",
        "    - Experimentation vs Persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYni0OdzQ1qS",
        "outputId": "f59f6a69-a5bb-4cd3-b69e-2f9dba7e97e0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "# TODO: validate that user works on Colab\n",
        "\n",
        "# Select your environment\n",
        "WORK_ENV = 1 # 0=Local 1=Drive\n",
        "\n",
        "CWD = os.getcwd()\n",
        "WORK_DIR_Lo = CWD+\"/onmt\"#\"/content/onmt\"\n",
        "\n",
        "if(WORK_ENV):\n",
        "    if os.path.exists(\"/content\"):\n",
        "        from google.colab import drive\n",
        "        WORK_DIR_Dr = \"/content/drive/MyDrive/onmt\"\n",
        "        drive.mount('/content/drive') # Mounting your Google drive\n",
        "    else:\n",
        "        print(\"Colab not detected. Reverting to local environment\")\n",
        "        WORK_DIR_Dr = WORK_DIR_Lo\n",
        "\n",
        "WORK_DIR = WORK_DIR_Dr if WORK_ENV else WORK_DIR_Lo\n",
        "DATA_DIR = f\"{WORK_DIR}/data\"\n",
        "\n",
        "if(not os.path.exists(WORK_DIR) and pathlib.Path(\".\").parent != \"onmt\"):\n",
        "    !mkdir -p $DATA_DIR\n",
        "    print(f\"Environment Created: {WORK_DIR}\")\n",
        "else:\n",
        "    print(\"Environment already exists...\")\n",
        "\n",
        "os.chdir(WORK_DIR) # Naviting to Work-environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54CbNv34Q1qT"
      },
      "source": [
        "### Activer le GPU\n",
        "\n",
        "- Édition > Paramètres du bloc-notes > GPU > Enregistrer\n",
        "- Exécution > GPU\n",
        "\n",
        "\n",
        "**IMPORTANT** : Gardez le GPU détaché jusqu'à ce que vous fassiez quelque chose qui nécessite des GPU (c'est-à-dire de l'entrainement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUmORoIBbyTi"
      },
      "outputs": [],
      "source": [
        "# GPU's information\n",
        "\n",
        "!nvidia-smi "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSZVdDz8Q1qU"
      },
      "source": [
        "## Préparation des données\n",
        "\n",
        "`rmaipkg` a des textes Bambara parallèles que nous utiliserons pour cette étape. Vous échangez facilement cela avec votre source de données.\n",
        "\n",
        "\n",
        "Sachez à l'avance : ***Le texte n'est pas entièrement nettoyé si vous rencontrez des problèmes, il est préférable de générer un échantillon différent***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pEFFe89CPfp",
        "outputId": "d2893939-b296-484d-cd21-4d813f77198c"
      },
      "outputs": [],
      "source": [
        "# Import the 'parralel' module from rmaipkg\n",
        "from rmai.datasets.text import parallel\n",
        "\n",
        "texts = parallel.get_text(max_len=10000, randomize=True) # Load texts in memory\n",
        "\n",
        "# Show first 10 lines\n",
        "print(texts[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSJ3G6z5Q1qV"
      },
      "source": [
        "- `parallel` : module pour les textes parallèles de RobotsMali\n",
        "- `parallèle.get_text` :\n",
        "    - retourne `max_len` nombre de lignes\n",
        "    - `randomize` lorsqu'il est défini, mélange les textes. Bon pour générer un ordre différent pour les textes à chaque exécution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "bYJ6O1jRQ1qW",
        "outputId": "f9bd8f1c-b79e-40e1-e56c-44df016cdff6"
      },
      "outputs": [],
      "source": [
        "\n",
        "train, valid = parallel.random_split(texts, 80) # Split Data into training and validation 80/20\n",
        "\n",
        "print(\n",
        "    \"Splitted correctly: \", \n",
        "    len(train)+len(valid) == len(texts)) # Sanity Check\n",
        " \n",
        "\"\"\"\n",
        "TASK I: Create the following variables containing unique language for each set\n",
        "- trainbam: train set bambara\n",
        "- trainfra: train set francais\n",
        "- validbam: valid set bambara\n",
        "- validfra: valid set francais\n",
        "\"\"\"\n",
        "\n",
        "# CODE HERE (Discuss in pseudo code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ite-dBrpQ1qX"
      },
      "source": [
        "- `parallel.random_split` : divise l'objet texte en ratios pour l'entraînement et la validation, il renvoie un type de tuple.\n",
        "    - Le premier argument est les \"textes\"\n",
        "    - Le deuxième argument est le taux auquel c'est divisé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zZRnj5XCQ1qX"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Task I - Solution\n",
        "x_extractor = lambda x, dset: [i[x] for i in dset]\n",
        "\n",
        "trainbam = x_extractor(0, train)\n",
        "trainfra = x_extractor(1, train)\n",
        "validbam = x_extractor(0, valid)\n",
        "validfra = x_extractor(1, valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs46PTx_Q1qY"
      },
      "source": [
        "- `x_extractor` est une fonction lambda pour la tâche de séparer les valeurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHLvrGbb7psQ"
      },
      "outputs": [],
      "source": [
        "# Optional Work - Further Cleaning the Data\n",
        "\n",
        "import string\n",
        "\n",
        "def clean_lines(lines):\n",
        "  nlines = []\n",
        "  for i in lines:\n",
        "    for j in string.punctuation + \"«»\":\n",
        "      if(not j in r\"!.:?()[]\"):\n",
        "        i = i.replace(j, \" \")\n",
        "    nlines.append(i)\n",
        "  return \" \".join(\"\".join(nlines).strip().split())\n",
        "\n",
        "trainbam = [clean_lines(i) for i in trainbam]\n",
        "trainfra = [clean_lines(i) for i in trainfra]\n",
        "validbam = [clean_lines(i) for i in validbam]\n",
        "validfra = [clean_lines(i) for i in validfra]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIH7wZMTQ1qZ"
      },
      "source": [
        "- Écrivons maintenant nos données dans des fichiers réels sur le système. `rmaipkg` a une méthode appelée `write_to` qui le fait pour nous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1UqyLq4n7kYl",
        "outputId": "82e028ed-991a-4b05-d64d-95df7d870b17"
      },
      "outputs": [],
      "source": [
        "parallel.write_to(lines=trainbam, name=\"src-train\", path=DATA_DIR)\n",
        "parallel.write_to(lines=trainfra, name=\"tgt-train\", path=DATA_DIR)\n",
        "parallel.write_to(lines=validbam, name=\"src-val\", path=DATA_DIR)\n",
        "parallel.write_to(lines=validfra, name=\"tgt-val\", path=DATA_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQIYPHHyQ1qa"
      },
      "source": [
        "Maintenant que nous avons écrit nos données et que nous avons un ensemble de données, adaptons-le maintenant à la traduction automatique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M6IYseaQ1qa"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_name = \"bam2fr\" # Name you are giving to your model\n",
        "config_name = f\"{WORK_DIR}/cool_config.yaml\"\n",
        "\n",
        "config = f\"\"\"\n",
        "\n",
        "# IndabaX - Mali 2022 (Configuration)\n",
        "\n",
        "save_data: {model_name}/run/{model_name}\n",
        "\n",
        "overwrite: True # Toggle this for rewritting\n",
        "\n",
        "data:  \n",
        "    robotsmali:\n",
        "        path_src: data/src-train.txt # DataPath\n",
        "        path_tgt: data/tgt-train.txt\n",
        "        transforms: []\n",
        "        weight: 1\n",
        "    valid:\n",
        "        path_src: data/src-val.txt\n",
        "        path_tgt: data/tgt-val.txt\n",
        "        transforms: []\n",
        "        weight: 1\n",
        "\n",
        "src_seq_length: 250\n",
        "tgt_seq_length: 250\n",
        "\n",
        "src_vocab: {model_name}/run/{model_name}.vocab.src.txt\n",
        "tgt_vocab: {model_name}/run/{model_name}.vocab.tgt.txt\n",
        "skip_empty_level: silent\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Write config file\n",
        "with open(config_name, \"w\") as fp:\n",
        "    fp.write(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AkOmTviQ1qb"
      },
      "source": [
        "Ici, nous avons créé un fichier de configuration `cool_config.yaml` contenant des informations générales sur le jeu de données. Qu'avons-nous fait:\n",
        "\n",
        "- Nous avons attribué le nom de notre modèle à une variable\n",
        "- Générez un fichier de configuration avec quelques détails de base tels que :\n",
        "    - où se trouvent les données\n",
        "    - où générer les fichiers de vocabulaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-riq5HkO3Kl",
        "outputId": "7c4a64d3-2b6a-4af0-ac08-a830500de367"
      },
      "outputs": [],
      "source": [
        "# OpenNMT Building the vocabulary\n",
        "\n",
        "!onmt_build_vocab -c $config_name -n_sample -1 --dump_samples # -1 full corpus, bpe, sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe34hfPxQ1qc"
      },
      "source": [
        "La commande `onmt_build_vocab` construit le vocabulaire nécessaire à la formation. Ici, nous l'avons appelé avec trois paramètres\n",
        "\n",
        "- `-c config_file` : chemin du fichier de configuration à utiliser pour le processus de création de vocabulaire\n",
        "\n",
        "- `-n_sample -1` : la construction d'un vocabulaire à l'aide du corpus complet peut remplacer **-1** par n'importe quel nombre, lorsque l'argument est omis, sa valeur par défaut est 5000.\n",
        "\n",
        "- `-- dump_sampes` : (argument facultatif) Écrire des échantillons lors de la construction du vocabulaire, trouvés dans `model_name/run/model_name`\n",
        "\n",
        "Nous n'avons pas besoin de construire un vocabulaire en utilisant le `onmt_build_vocab`, nous pouvons le construire nous-mêmes souvent cela est nécessaire, `tokenization` varie selon la langue, chaque langue a ses caractéristiques uniques. Un exemple de ceci est montré ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYamw9WKQ1qc",
        "outputId": "fca04ed2-db52-41c7-e0bc-e5fa960a26c9"
      },
      "outputs": [],
      "source": [
        "# Custom Tokenization (rm-daba-tokenizer) \n",
        "# - Shown as example, CELL can be skipped\n",
        "\n",
        "from rmai.utils import daba\n",
        "\n",
        "dabautil = daba.DabaUtils()\n",
        "dabautil.tokenize_line(trainbam+validbam)[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16UMoYNuQ1qc"
      },
      "source": [
        "[`Daba`](https://github.com/maslinych/daba) est un outil incroyable développé par des linguistes de l'INALCO. Daba est une ceinture à outils pour les linguistes, et par conséquent pour nous, les gens des sciences informatiques qui cherchons à travailler avec la langue bambara.\n",
        "\n",
        "- Si vous cherchez à travailler avec Bambara, nous vous le recommandons fortement.\n",
        "\n",
        "\n",
        "(Retour à OpenNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNxx3TUCQ1qd"
      },
      "source": [
        "### Entraînement\n",
        "\n",
        "\n",
        "#### Architecture d'un modèle\n",
        "\n",
        "Nous \"concevons\" le modèle à travers la configuration que nous avons générée précédemment. Dans la cellule de code ci-dessous, nous écrivons comment nous nous attendons à ce que notre modèle soit.\n",
        "\n",
        "*Bien que le fichier de configuration puisse être mis à jour et ajusté.* Nous limiterons notre modification à quelques options, car nous n'avons pas assez de temps pour en discuter.\n",
        "\n",
        "Vous pouvez appeler cette configuration comme ingrédients de la sorcière.\n",
        "\n",
        "- save_model : est le chemin dans lequel le point de contrôle de notre modèle est enregistré.\n",
        "- save_checkpoint_steps : X, à chaque étape X de l'entraînement, un point de contrôle est enregistré.\n",
        "- train_steps : Nombre d'étapes d'entraînement\n",
        "- valid_steps : exécuter la validation, toutes les X étapes\n",
        "- warmup_step : étape permettant d'ajuster le taux d'apprentissage après le début de l'entraînement.\n",
        "- GPU : basculer si vous utilisez GPU ou CPU\n",
        "\n",
        "BTW : Il s'agit d'une architecture de modèle de transformateur (transformer).\n",
        "\n",
        "Inspiré de :\n",
        "- [https://github.com/OpenNMT/OpenNMT-py/blob/master/config/config-transformer-base-1GPU.yml](https://github.com/OpenNMT/OpenNMT-py/blob/master/config/config-transformer-base-1GPU.yml)\n",
        "- Travail effectué par [A. A. Tapo](https://github.com/israaar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jJdJ145pQ1qd",
        "outputId": "ce951ba2-e3f0-471d-9383-15ab07fe85b3"
      },
      "outputs": [],
      "source": [
        "save_freq =  5 #100 # Checkpoint saving steps\n",
        "training_steps = 10 #500 # Number of steps for the training\n",
        "valid_steps = 2# 50 # Steps to validate training perfomance\n",
        "warmup_step = 3 #125 # Warmup 1/4th of training total\n",
        "GPU = 0 # 0: CPU or 1:GPU\n",
        "\n",
        "config += f\"\"\"\n",
        "\n",
        "# Training\n",
        "save_model: {model_name}/{model_name}\n",
        "save_checkpoint_steps: {save_freq}\n",
        "keep_checkpoint: 3 # Better for this activity\n",
        "seed: 1234\n",
        "train_steps: {training_steps}\n",
        "valid_steps: {valid_steps}\n",
        "warmup_steps: {warmup_step}\n",
        "report_every: {int(training_steps / 5)}\n",
        "external_evaluators: BLEU\n",
        "tensorboard: true\n",
        "tensorboard_log_dir: {model_name}/logs\n",
        "\n",
        "# Model\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "word_vec_size: 256 # Word embedding size\n",
        "hidden_size: 256\n",
        "layers: 6\n",
        "transformer_ff: 1024\n",
        "heads: 4\n",
        "dropout: 0.3\n",
        "\n",
        "# HyperParams\n",
        "accum_count: 8\n",
        "optim: adam\n",
        "adam_beta1: 0.9\n",
        "adam_beta2: 0.998\n",
        "decay_method: noam\n",
        "learning_rate: 0.0004\n",
        "max_grad_norm: 0.0\n",
        "\n",
        "batch_size: 4096\n",
        "batch_type: tokens\n",
        "normalization: tokens\n",
        "label_smoothing: 0.2\n",
        "\n",
        "max_generator_batches: 0\n",
        "\n",
        "param_init: 0.0\n",
        "param_init_glorot: 'true'\n",
        "position_encoding: 'true'\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "if(GPU):\n",
        "    config += \"\"\"\n",
        "world_size: 1 # CPU\n",
        "gpu_ranks: [0] # Change to number of GPU if more than 1 GPU\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "TASK II - Rewrite the config with this new configuration\n",
        "\"\"\"\n",
        "\n",
        "# CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0rlimCoQSJuK"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "with open(config_name, \"w\") as fp:\n",
        "    fp.write(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_C_ZPwJQ1qf"
      },
      "source": [
        "- **NOW LET THE MAGIC BEGIN**\n",
        "\n",
        "We will be running our experiment but prior to launching the training command we will start tensorboard. *Tensorboard* helps us visualize what is happening with our model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891
        },
        "id": "oA-mTF1VVMkp",
        "outputId": "25954538-ea03-41d6-c671-8e7e425a648c"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "%tensorboard --logdir {model_name}/logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `onmt_train` takes the configuration file as argument. Hopefully if we did everything prior, the training should start without any issue. Training duration depends on the training steps, computing power, and data_size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooPXDOZ8PYYN",
        "outputId": "d4c3aac7-7d42-41d5-97d8-0a268fee61c0"
      },
      "outputs": [],
      "source": [
        "!onmt_train -config $config_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Essai & Évaluation des performances [BLEU, Tensorboard]\n",
        "\n",
        "### Task 3\n",
        "- Create own test file (10 sentences)\n",
        "    - Create a new with file in the `data` directory then write 10 bambara lines onto the file.\n",
        "    - Create another file in the `data` directory then write the french translation for the bambara text file.\n",
        "\n",
        "### You can use our test set, if you feel lazy, or uninspired\n",
        "- Copy our test file from [Repo](https://github.com/robotsmali-ai/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Set\n",
        "!wget $PATH_TO_TEXTS -p data # Bam\n",
        "!wget $PATH_TO_TEXTS -p data # Fr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`onmt_translate` command is used to conduct translation with the trained model. Here we are using five parameters:\n",
        "- `model`: a model checkpoint file\n",
        "- `src` : our test file the source language\n",
        "- `output`: where we want to write our model's predictions\n",
        "- `gpu`: Number of GPU's to run on\n",
        "- `verbose`: Display the scores per prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVmk4Q1DSqBa",
        "outputId": "2e55afb5-872a-478a-f069-9df98346889f"
      },
      "outputs": [],
      "source": [
        "!onmt_translate -model {model_name}/b2f_step_1000.pt -src data/src-test.txt -output {model_name}/pred_{model_name}_1000.txt # -gpu -1 -verbose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Ta-da! Let's see how our model did, now.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!cat {model_name}/pred_{model_name}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bleu (BiLingual Evaluation Understudy) score is automatic metric to evaluate machine translation. It is useful, as it allows us to have a general idea about how our model's doing if we are working with a larger model, and larger datasize.\n",
        "\n",
        "In our case we will use our own translation as reference to determine how well our model did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmwFxAjTQ1qh"
      },
      "outputs": [],
      "source": [
        "!sacrebleu {reference} -i {predicion} -m bleu -b -w 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next step: Go back and ajust the params for your own personalization.\n",
        "\n",
        ">>> **Thank you for joining us. Come to the N.L.P event on Saturday to learn about NLP, ASR in details.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vslfOYyiQ1qh"
      },
      "source": [
        "<div>\n",
        "<img width=\"250\" src=\"https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img/https://indabax.robotsmali.org/wp-content/uploads/2022/11/robotsmali.png\" />\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.2 ('indabax-ml-practivcals')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "062df52f2add3985dc8f7857aa357ac7d0bf80c221fd747fc356e78d6365606f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
